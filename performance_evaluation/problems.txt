1. Are we usina huggingface tokenizer? but the manufacturer provides optimized tokenizer. shouldn't that be better?
2. um...the output is "low quality" for the model. Can this be related to the tokenizer not being the manufacturer's tokenizer?
3. The ax650 / llm8850 only ever seems to use about 27% NPU load. Is that a function of how we're calling it? Can we put the whole thing to work?
4. The chat completion is shockingly slow, many times slower than when using the manufacturer's runtime for the exact same optimized model. Is it because our implementation is in python and not C?